<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>RL in E</title>

	<link rel="stylesheet" href="css/reset.css">
	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css">
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css"
		integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous" />


	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/monokai.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<style>
		.reveal i {
			font-family: 'FontAwesome';
			font-style: normal;
		}

		.red {
			color: rgb(255, 150, 150);
		}

		.green {
			color: rgb(150, 255, 150);
		}

		.yellow {
			color: rgb(255, 255, 123);
		}

		.blue {
			color: rgb(150, 150, 255);
		}

		.row {
			display: flex;
			flex-direction: row;
			justify-content: space-around;
		}

		.box {
			border: 1px solid white;
			padding: 15px;
			margin: 5px;
			font-size: 24pt;
		}

		.emph {
			font-style: italic;
		}

		.ul {
			text-decoration: underline;
		}

		.smallfont {
			font-size: 0.8em;
		}

		.smallerfont {
			font-size: 0.7em;
		}
	</style>
	<div class="reveal">
		<div class="slides">
			<section>
				<h3>Reinforcement Learning for the <br>E Theorem Prover</h3>
				<hr>
				<h4>Jack McKeown & Geoff Sutcliffe</h4>
				<aside class="notes">
					Hi. My name is Jack McKeown and today I'll be talking about the work my advisor Geoff Sutcliffe and
					I have been working on
					and the approach we're taking to incorporate reinforcement learning into an automated theorem prover
					called "E".
				</aside>
			</section>

			<section id="outline">
				<h3>Outline</h3>
				<div class="box">
					<ol>
						<li class="fragment">First-Order Logic & CNF</li>
						<li class="fragment">Saturation-based ATP & Given Clause Selection</li>
						<li class="fragment">E heuristics</li>
						<li class="fragment">Reinforcement Learning & Proximal Policy Optimization</li>
						<li class="fragment">Experiments, Results, & Analysis</li>
						<li class="fragment">Future Research</li>
					</ol>
				</div>
				<aside class="notes">
					First, I'll give an overview of automated theorem proving and the problem we're trying to solve,
					then I'll give an overview of the relevant reinforcement learning concepts,
					and then I'll be able to explain how we are using reinforcement learning in our attempt to solve the
					theorem proving problem.

					I'll explain our experimental setup and the results we've obtained so far.
					and conclude with some analysis of our results and a discussion of potential directions for future
					research.
				</aside>
			</section>

			<section id="fol-cnf">
				<section id="prop">
					<h3>Propositional Logic</h3>
					<ul>
						<li>logical constants (usually $p$, $q$, etc...)</li>
						<li>logical connectives ($\&, |, \sim, \implies$)</li>
						<li>Example statement: <span class="green">$(p|q) \implies r$</span></li>
					</ul>
					<aside class="notes">
						Propositional logic is the simplest form of logic that 
						is typically presented in a discrete math class.
						There, you have logical constants which map to either true or false and are usually given names like 'p' or 'q'.
						You also have logical connectives like 'and', 'or', 'not', and implication.
						Statements in propositional logic evaluate to true or false based on the rules for the logical connectives and the truth values of the logical constants. 
					</aside>
				</section>
				<section id="fol">
					<h3>First Order Logic</h3>

					<ul style="font-size:0.8em;">
						<li class="fragment"><span class="green">$\forall X \exists Y$</span>: Variables &
							Quantification</li>
						<li class="fragment"><span class="green">$f(Y)$</span>: A <em
								style="text-decoration: underline;">term</em> formed by the application of a <em
								style="text-decoration: underline;">function</em> to a variable</li>
						<li class="fragment">Constants are functions of arity zero.</li>
						<li class="fragment">logical-constants can be viewed as predicates of arity-zero.</li>
					</ul>
					<span class="fragment">
						An example first order statement:
						<span class="green">
							$$ \forall X \exists Y : p(X,f(Y)) \; | \; (q(X)\wedge q(Y))$$
						</span>
					</span>

					<aside class="notes">
						First-order logic extends this by introducing variables that can be either existentially or
						universally quantified.
						Also, in addition to the *logical*-constants from before, there are now *non*-logical constants which refer to fixed elements of some
						"domain" of discourse. Functions between domain elements are also introduced, where non-logical constants can just be thought of as functions of arity zero.

						Since statements in first-order logic still have to be evaluated to true or false,
						these domain elements are eventually mapped to true or false values by some truth-valued function.
						These functions which output true or false are called predicates and logical-constants can then be thought of as predicates of arity zero.
					</aside>
				</section>

				<section id="cnf">
					<h3>Clause Normal Form</h3>
					<div class="box">
						This first order statement:
						<span class="red">
							$$ \forall X \exists Y : p(X,f(Y)) \; | \; (q(X)\wedge q(Y))$$
						</span>
						becomes
						<span class="green">
							$$ \underbrace{p(X, f(sk(X))) \; | \; q(X)}_{clause} \;\; \bigwedge \;\; \underbrace{p(X,
							f(sk(X))) \; | \;q(sk(X))}_{clause} $$
						</span>
						in Conjunctive/Clause Normal Form (CNF).<br>
						(In CNF all variables are universally quantified.)
					</div>

					<aside class="notes">
						For the sake of theorem proving in first-order logic, it is useful to convert statements into
						a special form called "clause normal form". 
						This is useful because the inference rules used by theorem provers operate on clauses.

						A first-order statement is in clause normal form if it is a conjunction of clauses 
						where a clause is a disjunction of literals and there are no existentially quantified variables.
						In other words, it's an "AND" of "OR"s with no existentially quantified variables.

						In order to remove existentially quantified variables, you can replace the variable with a term representing the 
						appropriate choice for the variable's value. (If "for all X", "there exists a Y" then replace Y with some function of X that picks the "one that exists".)
						This is called "skolemization" and the introduced function is called a "skolem function".
					</aside>
				</section>
			</section>

			<section id="saturation">
				<section>
					<h3>"Saturation-based" Theorem Proving</h3>
					<div style="text-align:left; font-size:0.85em;">
						<span class="green">Saturation:</span> computing the closure of a set of statements with respect
						to a complete set of inference rules.
						<span class="fragment">
							<hr>For a set consisting of axioms and the negation of an entailed conjecture, this closure
							<span class="emph" style="text-decoration: underline;">must</span> include <span
								class="red">false</span>
						</span>
						<span class="fragment">
							<hr>A <span class="green">conjecture</span> is therefore proved by searching for the empty
							clause by <span class="green">saturating the set $Ax\; \cup \sim C$</span>
						</span>
					</div>

					<aside class="notes">
						Although there are other approaches to theorem proving, the most effective provers currently are "saturation-based" provers.

						These provers work by making sound inferences from the union of a set of axioms and the negation of a conjecture
						in order to derive the empty clause which serves as an explicit witness of a contradiction.
					</aside>
				</section>

				<section id="gcs">
					<h3>Given Clause Selection</h3>
					<img src="images/processedAndUnprocessedSets.svg" style="background:whitesmoke">
					<aside class="notes">
						In practice, two sets of clauses are maintained instead of one.
						This is done to avoid considering all pairs of clauses when searching for an applicable inference rule.

						These sets are often called the "processed" and "unprocessed" sets of clauses.
						The processed set starts off empty, and the unprocessed set initially contains all of the axiom and negated conjecture clauses.
						
						Proof search proceeds by selecting a clause from the unprocessed set to bring into the processed set.
						The selected clause is called the "given clause" and the process of selecting it is called "given clause selection".
						The given clause then "reacts" with the clauses already in the processed set to produce new clauses via the inference rules.
						The inferred clauses are then added to the unprocessed set.
						Note that the given clause has to be checked against all clauses in the processed set, but the other clauses in the processed set
						don't have to be rechecked against each other for inferences. This is why maintaining two sets is useful.

						This loop proceeds until the empty clause is selected.
						A proof can then be constructed by tracing back through the tree of clauses that led to the empty clause.
					</aside>
				</section>

			</section>

			<section id="E">
				<h3>Given Clause Selection in E</h3>

				<p style="font-size:18pt">Example Clause Evaluation Function (CEF): <span
						class="green">Clauseweight(<span class="red">PreferGoals</span>, 1,1,1)</span></p>

				<div class="row">
					<textarea class="fragment" cols="50" rows="7" style="overflow: hidden;">
Example "heuristic" in E's --auto mode:

10 * Clauseweight(PreferGoals,1,1,1),
10 * Clauseweight(PreferNonGoals,1,1,1),
10 * Clauseweight(ByHornDist,1,1,1),
1 * FIFOWeight(ConstPrio)
						</textarea>

					<textarea class="fragment" cols="50" rows="7" style="overflow: hidden;">
Example "heuristic" in RL mode:

0.5 * Clauseweight(PreferGoals,1,1,1),
0.2 * Clauseweight(PreferNonGoals,1,1,1),
0.1 * Clauseweight(ByHornDist,1,1,1),
0.1 * FIFOWeight(ConstPrio),
...
						</textarea>

				</div>

				<aside class="notes">
					Since a proof is only found when the given clause selection loop ends up selecting the empty clause,
					selecting the right clauses is very important, but it is hard to say which clauses will lead to the derivation of the empty clause.
					This is the main research problem of this work.

					The way that E allows the user to influence the selection of clauses is through the use of "clause evaluation functions" or CEFs.
					These are different (but usually pretty simple) functions that are used to assign a score to a clause.
					They are efficiently implemented in E as priority queues over the clauses in the unprocessed set.

					E then allows users to specify a schedule over whatever CEFs you choose to use.
					E calls this schedule a "heuristic".

					In our work, instead of using a standard E heuristic, we consider using a neural network to output the parameters
					of a categorical distribution over the CEFs as a function of E's internal state.
				</aside>

			</section>


			<section>
				<section>
					<h3>RL / Policy Gradients</h3>
					<ul>
						<li class="fragment">States, Actions and Rewards</li>
						<li class="fragment" style="line-height: 1.8em;">Policy:
							<span class="green fragment box">$\pi(s) = a$</span>
							<span class="green fragment box">$\sum_a \pi(s,a) = 1$</span>
						</li>
						<li class="fragment">Goal: <span class="green">Maximize expected future discounted
								rewards</span></li>
						<li class="fragment" style="line-height: 2.1em;">Policy Gradient Loss:
							$\displaystyle\frac{1}{T}\sum_{t=1}^T [$
							<span class="green fragment"
								style="padding:1em 0em;font-size:0.8em;">$-\log{(\pi(s_t,a_t))}$</span>
							<span class="fragment">
								$\cdot $
								<span class="blue">$R_t$</span>
							</span>
							$]$
						</li>


						<li class="fragment" style="line-height: 2.1em;">Reducing Variance:
							$\displaystyle\frac{1}{T}\sum_{t=1}^T [$
							<span class="green fragment"
								style="padding:1em 0em;font-size:0.8em;">$-\log{(\pi(s_t,a_t))}$</span>
							<span class="fragment">
								$\cdot $
								<span class="red">$A_t$</span>
							</span>
							$]$
						</li>
						<li class="fragment">
							<span class="red">
								$A_t$
							</span>
							$=$
							<span class="blue">$R_t$</span> $- V(s_t)$
						</li>
					</ul>

					<aside class="notes">
						Reinforcement learning is a subset of machine learning concerned with taking actions from states
						to maximize the "rewards" received from some environment.

						Usually this is done without any knowledge of how states transition to other states or what the rewards
						will be for taking certain actions.
						Although there are many other ways to do reinforcement learning, we focus on policy gradients.
						The idea behind policy gradients is to randomly initialize a policy and then use gradient descent to
						adjust the policy parameters to make actions which precede high rewards more likely.

						You can see this in the loss function, where we minimize the negative log likelihood of
						the actions we took, weighted by the reward received.
						
						A trick that can be done to reduce the variance of the gradient is to subtract a learned value function
						from the reward. This adjusted reward is called the advantage and is the basis for methods like
						advantage actor critic and proximal policy optimization.
					</aside>
				</section>

				<section>
					<h3>Proximal Policy Optimization (PPO)</h3>
					<ul style="font-size:0.7em;">
						<li class="fragment">Simpler successor to Trust Region Policy Optimization (TRPO)</li>
						<li class="fragment">Main Idea: <span class="green">Learn from the same data multiple times in
								mini-batches <br>without drastically affecting the current policy.</span></li>
						<span class="fragment">
							<li>A special loss limits the gradient so that 
								<span class="green">$0.8\pi_{old}(s,a) \leq \pi(s,a) \leq 1.2\pi_{old}(s,a)$</span>
							</li>
							<ul>
								<li>$r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t |
									s_t)}$</li>
								<li>$L_{actor}(\theta) = \displaystyle\frac{1}{T}\sum_{t=1}^T [min($ 
									<span class="green">$r_t(\theta)A_t$</span> $, $ 
									<span class="red">$clip(r_t(\theta),1-\epsilon, 1+\epsilon)A_t$</span> $)]$
								</li>
							</ul>
						</span>
					</ul>
					<aside class="notes">
						Proximal policy optimization, like other actor-critic methods jointly learns two models:
						the policy (aka actor) and a critic that estimates the value function to reduce the policy gradient variance.

						PPO is a simpler alternative to trust region policy optimization and both of these models
						try to gain sample efficiency by performing multiple gradient updates on the same data.
						This is difficult to do because the policy is constantly changing, so the data is no longer
						representative of the current policy, making the gradient updates invalid.

						To allow for multiple gradient updates, TRPO and PPO both try to limit the magnitude of the
						overall policy change. Despite taking multiple gradient steps on the same data, the policy
						is constrained to be close to the original policy.
					</aside>
				</section>


				<section>
					<h3>RL in E</h3>
					<ul>
						<li>States:
							<span class="green">
								$($
								<span class="fragment">$t$</span>,
								<span class="fragment">$|P|$</span>,
								<span class="fragment">$|U|$</span>,
								<span class="fragment">$W(P)$</span>,
								<span class="fragment">$W(U)$</span>
								$)$
							</span>
						</li>
						<li>Actions: <span class="green fragment">choice from a fixed set of
								CEFs</span></li>
						<li>Rewards: <span class="green fragment">selection of clauses in the
								proof</span></li>
					</ul>
					<aside class="notes">
						To define given clause selection as a reinforcement learning problem, we need to define the
						states, actions, and rewards.

						The simple state representation we use consists of 5 features:
						the number of given clause selections performed so far, 
						the number of clauses in the processed and unprocessed sets, 
						and the average "weight" of the clauses in the processed and unprocessed sets.

						Instead of representing actions as a choice amongst a varying number of clauses in the unprocessed set
						we simplify the problem by only allowing the agent to choose from a fixed set of clause evaluation functions.
						The selected CEF, then selects the given clause.
						This means that there is always the same set of actions in every state.

						Finally, a reward is given for every action which directly selected a clause that ended up in the proof.
						If no proof is found before a time limit, then all rewards are zero.
					</aside>

				</section>
			</section>


			<section>
				<section>
					<h3>Experiment Architecture</h3>
					<img src="images/architecture.svg" style="background:whitesmoke">
					<aside class="notes"></aside>
				</section>

				<section>
					<h3>Approaches Compared</h3>
					<ul class="smallerfont">
						<li class="fragment" style="margin-bottom:10px;line-height:1.3em;">
							<span class="green" style="text-decoration:underline">--auto</span>:
							<span class="smallfont">E's mode that analyzes a problem to choose a fixed
								heuristic</span>
						</li>
						<li class="fragment" style="margin-bottom:10px;line-height:1.3em;">
							<span class="green" style="text-decoration:underline">Round Robin</span>:
							<span class="smallfont">
								A round-robin schedule over the 20 chosen CEFs
								<br><span class="red">(A normal E heuristic with all ones for weights)</span>
							</span>
						</li>
						<li class="fragment" style="margin-bottom:10px;line-height:1.3em;">
							<span class="green" style="text-decoration:underline">Learned Categorical</span>:
							<span class="smallfont">
								Randomly samples from a learned categorical distribution
								<br><span class="red">(Ignores the RL state entirely)</span>
							</span>
						</li>
						<li class="fragment" style="margin-bottom:10px;line-height:1.3em;">
							<span class="green" style="text-decoration:underline">Distilled Categorical</span>:
							<span class="smallfont">
								The previous policy reduced into a standard E heuristic.
								<br><span class="red">(Removes the randomness and named pipe overhead)</span>
							</span>
						</li>
						<li class="fragment" style="margin-bottom:10px;line-height:1.3em;">
							<span class="green" style="text-decoration:underline">Neural Network</span>:
							<span class="smallfont">A shallow (3-layer) neural network policy</span>
						</li>
					</ul>

					<aside class="notes">
						(READ AND DESCRIBE SLIDE.)
					</aside>

				</section>

				<section>
					<h3>Experiment Details</h3>
					<ul class="smallerfont">
						<li class="fragment">CPU Limit for proof attempts: <span class="green">60 seconds</span></li>
						<li class="fragment"><span class="green">75 seconds</span> were given when testing the RL models
							to account for latency caused by the named pipe communication</li>
					</ul>
					<aside class="notes">
						(READ AND DESCRIBE SLIDE.)
					</aside>
				</section>

				<section>
					<h3>Results</h3>
					<table style="font-size:0.7em;">
						<thead>
							<tr>
								<th></th>
								<th style="text-align:right"><span class="green">--auto</span></th>
								<th style="text-align:right"><span class="green">Round Robin</span></th>
								<th style="text-align:right"><span class="green">Learned Categorical</span></th>
								<th style="text-align:right"><span class="green">Distilled Categorical</span></th>
								<th style="text-align:right"><span class="green">Neural Network</span></th>
							</tr>
						</thead>
						<tbody>
							<tr class="fragment">
								<td style="text-align:left"><span class="green">Problems Proved</span></td>
								<td style="text-align:right">228.2</td>
								<td style="text-align:right">232.0</td>
								<td style="text-align:right">231.6</td>
								<td style="text-align:right"><span class="green">232.2</span></td>
								<td style="text-align:right">231.3</td>
							</tr>
							<tr class="fragment">
								<td style="text-align:left"><span class="green">Given Clauses</span></td>
								<td style="text-align:right">4407.8</td>
								<td style="text-align:right">2329.0</td>
								<td style="text-align:right">2377.4</td>
								<td style="text-align:right">2262.6</td>
								<td style="text-align:right"><span class="green">2013.0</span></td>
							</tr>
							<tr class="fragment">
								<td style="text-align:left"><span class="green">Fewer Given Clauses than --auto</span>
								</td>
								<td style="text-align:right">0</td>
								<td style="text-align:right">1895.6</td>
								<td style="text-align:right">1743.6</td>
								<td style="text-align:right"><span class="green">1899.0</span></td>
								<td style="text-align:right">1897.6</td>
							</tr>
						</tbody>
					</table>
					<aside class="notes">
						So we did some experiments using the MPTPTP2078 dataset and we did 5-fold cross validation.

						The first thing we looked at was the number of problems solved by each approach (averaged across the 5 folds).
						They all solved roughly the same number of problems, with the distilled categorical model solving the most.

						Next we looked at the number of given clauses it took to find the proofs that that approach found.
						The neural network policy found proofs with the fewest given clauses on average.
						This suggests that it may be searching for proofs more efficiently than the other approaches.

						Finally, for each problem solved by both auto and the other approaches, 
						we looked at how many fewer given clauses each approach took to find a proof.
					</aside>

				</section>

				<section>
					<h3>Analysis of Actor
						<span class="green">(MPT1152+1.p)</span>
					</h3>
					<div class="row" style="justify-content: space-around;opacity:0.8;">
						<img class="fragment" src="images/actor1.png" style="scale:0.95;">
						<img class="fragment" src="images/actor2.png" style="scale:0.95;">
					</div>
					<aside class="notes">
						This graph shows the probability of each CEF being selected by the actor from the initial state of a particular problem.
						The x-axis here is training time.
						As you can see, the actor learns to heavily prefer one CEF to use at the beginning of the proof attempt.

						This second graph shows the probability of each CEF being selected by the actor during a proof attempt using the fully trained model.
						The x-axis here is therefore the given clause selection index.

						As you can see, while the actor learns to heavily prefer one clause for the beginning of a proof attempt,
						it also learns to be more "fair" in its action choices as a proof attempt proceeds.
					</aside>

				</section>

				<section>
					<h3>Analysis of Critic</h3>
					<div class="row">
						<img src="images/critic.png">
					</div>
					<aside class="notes">
						These histograms show the critic evaluation of the initial states of the problems in the MPTPTP2078 dataset.
						The green histogram shows the critic evaluation for problems that were able to be solved
						and the red histogram shows the critic evaluation for problems that were not able to be solved.
						
						As shown by the rightward shift of the green histogram, the critic has learned 
						something about the likelihood of finding a proof even from the initial state.
					</aside>
				</section>
			</section>


			<section>
				<h3>Future Research</h3>
				<ul>
					<li class="fragment">Use the critic inside E with Monte-Carlo Tree Search</li>
					<li class="fragment">If that works well, learn actor and critic using MCTS as in
						AlphaGo/AlphaZero/MuZero</li>
				</ul>
				<aside class="notes"></aside>
			</section>


			<section>
				<h3>Thanks for listening</h3>
				<em
					class="green">Questions?</em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
				<em class="green">Suggestions?</em><br>
			</section>

		</div>
	</div>

	<script src="js/reveal.js"></script>

	<script>
		// More info about config & dependencies:
		// - https://github.com/hakimel/reveal.js#configuration
		// - https://github.com/hakimel/reveal.js#dependencies
		Reveal.initialize({
			hash: true,
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				config: 'TeX-AMS_HTML-full', // See http://docs.mathjax.org/en/latest/config-files.html
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},

			dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/highlight/highlight.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }
			]
		});


		document.addEventListener("keypress", function (e) {
			if (e.key == 'u') {
				document.querySelector("#outline").classList.add("floatingOutline")
			}
		})

	</script>
</body>

</html>